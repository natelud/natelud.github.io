---
layout: archive
title: "Research"
permalink: /research/
author_profile: true
---

My research focuses on developing intelligent robots that can effectively collaborate with humans in shared environments. I take a neuro-symbolic approach, combining the strengths of deep learning with structured knowledge representations to create more interpretable, sample-efficient, and robust robotic systems.

## Current Research

### Knowledge Graph-Augmented Reinforcement Learning (KG-RL)

I'm developing methods that integrate knowledge graphs with deep reinforcement learning to improve robot learning in collaborative scenarios. By incorporating symbolic knowledge about tasks, objects, and human behaviors, robots can learn more efficiently and make more interpretable decisions. This work is being applied to multi-agent collaboration environments and is part of an ongoing collaboration with the Honda Research Institute.

**Key contributions:**
- Novel architectures for integrating graph neural networks with RL policies
- Methods for leveraging prior task knowledge to accelerate learning
- Applications to collaborative cooking scenarios in Overcooked-AI
- Preparing submission to AAMAS 2026

### Temporal Knowledge Graphs for RL (TKG-RL)

Building on the KG-RL framework, I'm exploring how temporal knowledge graphs can help robots reason about dynamic environments and predict human behaviors over time. This work addresses the challenge of representing and utilizing time-varying relationships in collaborative robotics.

**Current focus:**
- Explicit temporal representations for evolving task states
- Strategy learning using LSTM-based variational autoencoders
- Integration with model predictive path integral (MPPI) controllers
- Connections to recent work on runtime temporal inference (RTTI)

## Past Research Projects

### Human Driver Modeling for Autonomous Vehicles
*CMU DRIVE Lab | RISS Fellowship 2023*

Developed models of human driver behavior to improve autonomous vehicle prediction and decision-making systems. This work resulted in a publication at ICRA 2024 and demonstrated how understanding human behavior patterns can enhance safety in mixed autonomy settings.

### Dynamic Human-Robot Co-Manipulation
*BYU RAD Lab | 2020-2023*

Investigated control strategies for robots that physically collaborate with humans in manipulation tasks. Focused on developing systems that can safely share forces with human partners while accomplishing shared goals, with applications to assistive technologies and collaborative manufacturing.

### Automated Agricultural Phenotyping
*BYU Capstone Project*

Designed and built an automated robotic system for measuring maize stalk mechanical properties. The system enables high-throughput phenotyping for agricultural research, with results currently under review at IEEE Sensors.

## Research Interests

I'm broadly interested in:

- **Physical Human-Robot Interaction**: How can robots safely and naturally interact through touch and force sharing?
- **Neuro-symbolic AI**: How can we combine learning with structured knowledge for better robustness and interpretability?
- **Multi-agent Systems**: How do we enable effective coordination between robots and humans in shared tasks?
- **Assistive Robotics**: How can robots fade into the background to help people in everyday life?
- **Safety-critical Control**: How do we guarantee safe behavior in uncertain, dynamic environments with humans?

## Collaborations

- **Honda Research Institute**: Neuro-symbolic approaches for robotic collaboration
- **CMU AART Lab**: Core research on KG-RL and TKG-RL methods
- **CMU DRIVE Lab**: Human-robot interaction in autonomous driving contexts

---

*I'm actively seeking PhD positions for Fall 2026 in labs working on physical human-robot interaction. If you're interested in collaboration or have questions about my research, please reach out!*
